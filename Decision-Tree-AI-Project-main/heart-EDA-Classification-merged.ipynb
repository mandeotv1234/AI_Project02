{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# IMPORT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "from pathlib import Path\n",
    "import os\n",
    "import pandas as pd\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# READFILE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_names = [\n",
    "    'age', 'sex', 'cp', 'trestbps', 'chol', 'fbs',\n",
    "    'restecg', 'thalach', 'exang', 'oldpeak', 'slope',\n",
    "    'ca', 'thal', 'num'\n",
    "]\n",
    "\n",
    "DATA = pd.read_csv(\n",
    "    'Data/processed.cleveland.csv',\n",
    "\tencoding = 'latin1',\n",
    "    names = col_names,\n",
    "    header = None,\n",
    ")\n",
    "\n",
    "DATA.replace('?', pd.NA, inplace=True)\n",
    "\n",
    "DATA['num'] = DATA['num'].apply(lambda x: 1 if x>0 else 0)\n",
    "\n",
    "DATA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA.describe()\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "**Variable Description**<br>\n",
    "1. age: age in years<br>\n",
    "2. sex: gender of patient<br>\n",
    "3. cp: chest pain type<br>\n",
    "4. trestbps: resting blood pressure\n",
    "5. chol: serum cholestoral in mg/dl\n",
    "6. fbs: fasting blood sugar > 120 mg/dl\n",
    "7. restecg: resting electrocardiographic results\n",
    "8. thalach: maximum heart rate achieved\n",
    "9. exang: exercise induced angina\n",
    "10. oldpeak: ST depression induced by exercise relative to rest\n",
    "11. slope: the slope of the peak exercise ST segment\n",
    "12. ca: number of major vessels (0-3) colored by flourosopy\n",
    "13. thal: 3 = normal; 6 = fixed defect; 7 = reversable defect\n",
    "14. num: diagnosis of heart disease\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# ANYLIST FEATURES\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(DATA.columns.values)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "**Categorial Features**<br>\n",
    "Categorial :  'sex', 'fbs', and 'exang'.<br>\n",
    "Ordinal: 'cp', 'restecg', 'slope', 'ca', 'thal' and 'num'.<br><br>\n",
    "**Numeric Features**<br>\n",
    "Continous: 'oldpeak'<br>\n",
    "Discrete: 'age', 'trestbps', 'chol' and 'thalach'<br><br>\n",
    "\n",
    "The question is is there any error features?\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# DATA PREPROCESSING\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Missing Values per Column:\")\n",
    "print(DATA.isnull().sum())\n",
    "print(\"------------------------------------\")\n",
    "print(\"Infinity Values per Column:\")\n",
    "print((DATA == np.inf).sum() + (DATA == -np.inf).sum())\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "The answers is **YES**, There are some patients don't have 'ca' or 'thal' on this dataset. We will erase those all rows and collumns that is contains an error value.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = DATA.dropna()\n",
    "df_clean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(data = df_clean,vars=['age', 'chol', 'trestbps','thalach','num'])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_columns = ['age', 'chol', 'trestbps','thalach','num']\n",
    "corr = DATA[selected_columns].corr()\n",
    "\n",
    "# Vẽ heatmap\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(corr, annot=True, cmap='Blues', fmt=\".2f\", square=True)\n",
    "plt.title('Heatmap of Feature Correlations')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# ANYLIST CATEGORICAL DATA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def histogram(variable):\n",
    "    \"\"\"\n",
    "    input: variable ex:\"Age\"\n",
    "    output: histogram & value count\n",
    "    \"\"\"\n",
    "    # get feature\n",
    "    var = DATA[variable]\n",
    "\n",
    "    # count number of continuous variable\n",
    "    varValue = var.value_counts()\n",
    "\n",
    "    # visualize\n",
    "    plt.figure(figsize=(8,3))\n",
    "    plt.hist(var, bins=20, color='blue', alpha=0.7)\n",
    "    plt.xlabel(variable)\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.title(variable)\n",
    "    plt.show()\n",
    "\n",
    "    print(\"{}: \\n{}\".format(variable, varValue))\n",
    "\n",
    "def bar_plot(variable):\n",
    "    \"\"\"\n",
    "    input: variable ex:\"Sex\"\n",
    "    output: bar plot & value count\n",
    "    \"\"\"\n",
    "    # get feature\n",
    "    var = DATA[variable]\n",
    "\n",
    "    # count number of categorical variable\n",
    "    varValue = var.value_counts()\n",
    "\n",
    "    # visualize\n",
    "    plt.figure(figsize=(8,3))\n",
    "    plt.bar(varValue.index, varValue)\n",
    "    plt.xticks(varValue.index, varValue.index.values)\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.title(variable)\n",
    "    plt.show()\n",
    "\n",
    "    print(\"{}: \\n{}\".format(variable, varValue))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category1=['sex', 'fbs', 'exang','cp','restecg', 'slope', 'ca', 'thal', 'num']\n",
    "for c in category1:\n",
    "    bar_plot(c)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# ANYLIST NUMERICAL DATA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_hist(variable):\n",
    "    plt.figure(figsize=(8,3))\n",
    "    plt.hist(DATA[variable],bins=50)\n",
    "    plt.xlabel(variable)\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.title(\"{} Distribituon with hist\".format(variable))\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numericVar=['oldpeak', 'age', 'trestbps', 'chol', 'thalach']\n",
    "for n in numericVar:\n",
    "    plot_hist(n)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# BASIC DATA ANYLIST\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "In this section I will use some Categorial Features to anylist and create a simple rating table for those features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA[[\"sex\",\"num\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA[[\"sex\",\"num\"]].groupby([\"sex\"],as_index = False).mean().sort_values(by='num', ascending=False)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# HEART DISEASE CLASSIFICATION WITH DECISION TREE\n",
    "\n",
    "## From EDA to Classification\n",
    "\n",
    "Based on the Exploratory Data Analysis above, we observed several key insights:\n",
    "\n",
    "1. **Missing Data**: The dataset has missing values in 'ca' and 'thal' columns that need to be handled\n",
    "2. **Feature Correlations**: The correlation heatmap showed relationships between features like age, cholesterol, and target variable\n",
    "3. **Data Distribution**: We analyzed both categorical and numerical features to understand their distributions\n",
    "4. **Feature Importance**: Some features like 'sex', 'exang', 'cp' showed strong correlations with heart disease\n",
    "\n",
    "Now we will use these insights to build and evaluate machine learning models for heart disease prediction.\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# Import Classification Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import graphviz\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
    "from sklearn.tree import export_graphviz\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "from IPython.display import display, Markdown, Image\n",
    "from IPython.display import Markdown, display as ds\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import plot_tree\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# READFILE AND PREPROCESSING FOR CLASSIFICATION\n",
    "\n",
    "## Data Preparation Based on EDA Findings\n",
    "\n",
    "From our EDA analysis, we identified that:\n",
    "- Missing values exist in 'ca' and 'thal' columns\n",
    "- Categorical variables need to be encoded for machine learning\n",
    "- The target variable 'num' needs to be converted to binary classification (0/1)\n",
    "\n",
    "Let's prepare our data accordingly:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_names = [\n",
    "    'age', 'sex', 'cp', 'trestbps', 'chol', 'fbs',\n",
    "    'restecg', 'thalach', 'exang', 'oldpeak', 'slope',\n",
    "    'ca', 'thal', 'num'\n",
    "]\n",
    "\n",
    "df = pd.read_csv(\n",
    "    r'Data/processed.cleveland.csv',\n",
    "\tencoding = 'latin1',\n",
    "    names = col_names,\n",
    "    header = None,\n",
    "    na_values = '?'\n",
    ")\n",
    "\n",
    "y = df['num'].apply(lambda x: 1 if x > 0 else 0)\n",
    "X = df.drop('num', axis = 1)\n",
    "\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(\n",
    "    subset=[\n",
    "        'age','sex','cp','trestbps','chol','fbs',\n",
    "        'restecg','thalach','exang','oldpeak','slope','ca','thal'\n",
    "    ],\n",
    "    inplace=True\n",
    ")\n",
    "\n",
    "df['num'] = df['num'].apply(lambda x: 1 if x>0 else 0)\n",
    "\n",
    "df = pd.get_dummies(\n",
    "    df,\n",
    "    columns=['sex','cp','restecg','slope','thal'],\n",
    "    drop_first=True\n",
    ")\n",
    "\n",
    "df\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# TRAIN SPLIT TEST\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratios = [\n",
    "    (0.4, 0.6),\n",
    "    (0.6, 0.4),\n",
    "    (0.8, 0.2),\n",
    "    (0.9, 0.1),\n",
    "]\n",
    "\n",
    "subsets = {}\n",
    "\n",
    "for tr, te in ratios:\n",
    "    X_tr, X_te, y_tr, y_te = train_test_split(\n",
    "        X, y, \n",
    "        train_size = tr, \n",
    "        stratify = y, \n",
    "        random_state = 42\n",
    "    )\n",
    "    key = f'{int(tr * 100)}/{int(te * 100)}'\n",
    "    subsets[key] = {\n",
    "        'X_train': X_tr, 'y_train': y_tr,\n",
    "        'X_test' : X_te, 'y_test' : y_te,\n",
    "    }\n",
    "\n",
    "print(\"Finish to create subsets:\", list(subsets.keys()))\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# TRAIN MODEL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_dict = {}\n",
    "\n",
    "# Train models for different train/test splits\n",
    "for key in ['40/60', '60/40', '80/20', '90/10']:\n",
    "    data = subsets[key]\n",
    "    clf = DecisionTreeClassifier(criterion='entropy', random_state=42)\n",
    "    clf.fit(data['X_train'], data['y_train'])\n",
    "    clf_dict[key] = clf\n",
    "    print(f\"Trained model for split {key}\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# EVALUATION METRICS\n",
    "\n",
    "## Model Performance Analysis\n",
    "\n",
    "Now let's evaluate how well our Decision Tree models perform on different train/test splits. This will help us understand:\n",
    "- Which train/test ratio works best for our dataset\n",
    "- How consistent our model predictions are\n",
    "- The trade-off between training data size and generalization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate all models\n",
    "for key, clf in clf_dict.items():\n",
    "    data = subsets[key]\n",
    "    y_pred = clf.predict(data['X_test'])\n",
    "    y_true = data['y_test']\n",
    "    \n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    misclassified_count = (y_pred != y_true).sum()\n",
    "    total_samples = len(y_true)\n",
    "    misclassified_rate = 100 * misclassified_count / total_samples\n",
    "    \n",
    "    print(f\"=== Split {key} ===\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Numbers of Wrong labels: {misclassified_count}/{total_samples}\")\n",
    "    print(f\"Ratio of wrong labels: {misclassified_rate:.2f}%\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# EXTENDED WORK : RANDOM FOREST CLASSIFICATION\n",
    "\n",
    "## Improving Performance with Ensemble Methods\n",
    "\n",
    "From our Decision Tree analysis, we can see the performance varies across different splits. Let's try Random Forest, which typically provides better performance by:\n",
    "- Reducing overfitting through ensemble learning\n",
    "- Handling feature interactions better\n",
    "- Providing more stable predictions\n",
    "\n",
    "Random Forest combines multiple decision trees to make final predictions, potentially improving upon our single Decision Tree results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = '40/60'\n",
    "data = subsets[key]\n",
    "\n",
    "# Train Random Forest\n",
    "clf_rf_40_60 = RandomForestClassifier(n_estimators=10, random_state=42)\n",
    "clf_rf_40_60.fit(data['X_train'], data['y_train'])\n",
    "\n",
    "# Predict\n",
    "y_pred_rf = clf_rf_40_60.predict(data['X_test'])\n",
    "\n",
    "# Confusion Matrix\n",
    "cm_rf = confusion_matrix(data['y_test'], y_pred_rf)\n",
    "print(f\"Confusion Matrix (Random Forest - {key}):\")\n",
    "print(cm_rf)\n",
    "\n",
    "# Classification Report\n",
    "print(f\"Classification Report (Random Forest - {key}):\")\n",
    "print(classification_report(data['y_test'], y_pred_rf))\n",
    "\n",
    "# Calculate accuracy and misclassification rate\n",
    "y_true = data['y_test']\n",
    "accuracy = accuracy_score(y_true, y_pred_rf)\n",
    "misclassified_count = (y_pred_rf != y_true).sum()\n",
    "total_samples = len(y_true)\n",
    "misclassified_rate = 100 * misclassified_count / total_samples\n",
    "\n",
    "print(f\"Numbers of Wrong labels: {misclassified_count}/{total_samples}\")\n",
    "print(f\"Ratio of wrong labels: {misclassified_rate:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# REFERENCES\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# CONCLUSION AND SUMMARY\n",
    "\n",
    "## Key Findings from our Analysis\n",
    "\n",
    "### From EDA:\n",
    "1. **Data Quality**: Dataset has 303 samples with missing values in 'ca' (4) and 'thal' (2) columns\n",
    "2. **Feature Insights**: \n",
    "   - Males show higher disease rates than females\n",
    "   - Exercise-induced angina correlates with higher disease probability\n",
    "   - Chest pain type and other cardiac features show varying disease associations\n",
    "3. **Data Distribution**: Both numerical and categorical features show distinct patterns for disease vs non-disease cases\n",
    "\n",
    "### From Classification:\n",
    "1. **Model Performance**: Decision Tree models achieved varying accuracy across different train/test splits\n",
    "2. **Random Forest Improvement**: Ensemble method typically shows better generalization than single Decision Tree\n",
    "3. **Feature Importance**: The models confirm insights from EDA about which features are most predictive\n",
    "\n",
    "### Workflow Connection:\n",
    "This analysis demonstrates a complete machine learning pipeline:\n",
    "**EDA → Data Understanding → Feature Engineering → Model Training → Evaluation → Insights**\n",
    "\n",
    "The EDA phase informed our preprocessing decisions, while the classification results validate our exploratory findings about feature importance and data patterns.\n",
    "\n",
    "---\n",
    "\n",
    "## References\n",
    "\n",
    "**EDA References:**<br>\n",
    "https://machinelearningcoban.com/tabml_book/ch_data_processing/eda.html<br><br>\n",
    "Source Dataset: https://archive.ics.uci.edu/dataset/45/heart+disease<br><br>\n",
    "\n",
    "**Classification References:**<br>\n",
    "https://machinelearningcoban.com/tabml_book/ch_model/random_forest.html<br>\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# ANYLIST CATEGORICAL DATA\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
